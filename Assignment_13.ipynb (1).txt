{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function\n",
        "       \n",
        "          Decision Tree\n",
        "\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans Decision Tree in Classification\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm that splits data into branches based on feature values to make predictions.\n",
        "\n",
        "It is shaped like a tree (root → branches → leaves).\n",
        "\n",
        "In classification, the leaves represent class labels (e.g., Spam vs. Not Spam).\n",
        "\n",
        "How it Works\n",
        "\n",
        "1. Start at the Root Node\n",
        "\n",
        "The tree begins with all training data at the root.\n",
        "\n",
        "2. Splitting Criteria\n",
        "\n",
        "The algorithm decides the best feature to split data.\n",
        "\n",
        "Common criteria:\n",
        "\n",
        "Gini Index → measures impurity of a node.\n",
        "\n",
        "Entropy / Information Gain → measures reduction in uncertainty.\n",
        "\n",
        "3. Recursive Splitting\n",
        "\n",
        "Data is split into subsets until:\n",
        "\n",
        "All data in a node belongs to one class, OR\n",
        "\n",
        "A stopping condition is reached (e.g., max depth, min samples).\n",
        "\n",
        "4. Leaf Node Prediction\n",
        "\n",
        "Each leaf node represents the final class label (majority class of samples in that node).\n",
        "\n",
        ". Example (Email Classification)\n",
        "\n",
        "Imagine we are classifying emails as Spam (1) or Not Spam (0):\n",
        "\n",
        "Root Node:\n",
        "\n",
        "Feature = \"Contains word 'Offer'?\"\n",
        "\n",
        "If Yes → Go right branch (Spam)\n",
        "\n",
        "If No → Go left branch\n",
        "\n",
        "Next split (left branch):\n",
        "\n",
        "Feature = \"Contains word 'Meeting'?\"\n",
        "\n",
        "If Yes → Not Spam\n",
        "\n",
        "If No → Spam\n",
        "\n",
        "At the end, the leaf nodes give predictions: Spam or Not Spam.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans Impurity Measures in Decision Trees\n",
        "Decision Trees split data by finding the \"best\" feature.\n",
        "The \"best\" means the feature that produces the purest child nodes.\n",
        "To measure purity/impurity, two main metrics are used: Gini Impurity and Entropy (Information Gain).\n",
        "\n",
        "1️⃣ Gini Impurity\n",
        "\n",
        "Definition: Probability of misclassifying a randomly chosen sample if it were labeled according to the class distribution in that node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Gini = 1 - \\sum_{i=1}^C p_i^2\n",
        "\n",
        "Where:\n",
        "\n",
        " = probability of class  at that node\n",
        "\n",
        " = number of classes\n",
        "\n",
        "Range: 0 (pure node) → 0.5 (for binary, max impurity).\n",
        "\n",
        "Interpretation: Lower Gini means purer split.\n",
        "\n",
        "Example:\n",
        "If a node has 70% spam (class 1) and 30% not spam (class 0):\n",
        "\n",
        "Gini = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42\n",
        "\n",
        "2️⃣ Entropy\n",
        "\n",
        "Definition: Measures the amount of \"uncertainty\" or \"disorder\" in the node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Entropy = - \\sum_{i=1}^C p_i \\log_2(p_i)\n",
        "\n",
        "Where:\n",
        "\n",
        " = probability of class .\n",
        "\n",
        "Range: 0 (pure node) → 1 (for binary, max impurity at 50/50 split).\n",
        "\n",
        "Example:\n",
        "Same node with 70% spam, 30% not spam:\n",
        "\n",
        "Entropy = -(0.7 \\log_2 0.7 + 0.3 \\log_2 0.3)\n",
        "\n",
        "= -(0.7 \\times -0.5146 + 0.3 \\times -1.737) = 0.881 \n",
        "\n",
        "3️⃣ How They Impact Splits\n",
        "\n",
        "At each split, the algorithm calculates impurity before and after the split.\n",
        "\n",
        "Then, it selects the feature that produces the highest reduction in impurity.\n",
        "For Gini: Pick the split with the lowest weighted Gini.\n",
        "\n",
        "For Entropy: Pick the split with the highest Information Gain (reduction in entropy).\n",
        "\n",
        "Both usually give similar results, but:\n",
        "Gini is slightly faster (no log computation).\n",
        "\n",
        "Entropy is more “theoretical,” rooted in information theory.\n",
        "\n",
        "4️⃣ Intuitive Difference\n",
        "\n",
        "Gini: Focuses on maximizing purity (how mixed classes are).\n",
        "\n",
        "Entropy: Focuses on information gain (how much uncertainty is reduced).\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans Pruning in Decision Trees\n",
        "\n",
        "Decision Trees tend to overfit (become too deep, capturing noise instead of patterns).\n",
        "To solve this, we use pruning techniques:\n",
        "\n",
        "1️⃣ Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition: Stop growing the tree before it becomes too complex.\n",
        "\n",
        "How? Set conditions like:\n",
        "\n",
        "max_depth → limit depth of tree\n",
        "\n",
        "min_samples_split → minimum samples required to split a node\n",
        "\n",
        "min_samples_leaf → minimum samples in a leaf\n",
        "\n",
        "max_leaf_nodes → maximum number of leaf nodes\n",
        "\n",
        "Essentially, we restrict tree growth upfront.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Faster training and prevents overfitting early.\n",
        "\n",
        "Example: In spam detection, pre-pruning avoids splitting nodes on very rare words that don’t generalize well.\n",
        "\n",
        "2️⃣ Post-Pruning (Cost Complexity Pruning)\n",
        "\n",
        "Definition: First grow a full tree, then prune (cut back) unnecessary branches.\n",
        "\n",
        "How?\n",
        "\n",
        "Build a deep tree.\n",
        "\n",
        "Use a pruning technique (e.g., Reduced Error Pruning or Cost Complexity Pruning in scikit-learn) to remove branches that don’t improve performance on validation data.\n",
        "\n",
        " Essentially, we grow first, then simplify.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Produces a smaller, simpler tree that still performs well.\n",
        "\n",
        "Example: In medical diagnosis, post-pruning can remove branches that are too specific to rare patient cases, making the model more interpretable\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans Information Gain in Decision Trees\n",
        "\n",
        "1️⃣ Definition\n",
        "\n",
        "Information Gain (IG) measures the reduction in uncertainty (entropy) after a dataset is split on a feature.\n",
        "\n",
        "It tells us how much \"information\" a feature gives about the class label.\n",
        "\n",
        "In Decision Trees, Information Gain helps us decide which feature to split on at each step.\n",
        "\n",
        "2️⃣ Formula\n",
        "\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
        "\n",
        "Where:\n",
        "\n",
        " = original dataset\n",
        "\n",
        " = feature being considered for split\n",
        "\n",
        " = subset of  where feature  has value\n",
        "\n",
        " = proportion of samples in that subset\n",
        "\n",
        "So:\n",
        "\n",
        "Entropy(S) = impurity before the split\n",
        "\n",
        "The second part = weighted impurity after the split\n",
        "\n",
        "Information Gain = impurity reduction\n",
        "\n",
        "3️⃣ Why Important?\n",
        "\n",
        "At each node, the Decision Tree chooses the feature with the highest Information Gain.\n",
        "\n",
        "This ensures each split creates purer child nodes (less mixed classes).\n",
        "\n",
        "Without IG (or Gini), splits would be random and the tree would be meaningless.\n",
        "\n",
        "4️⃣ Example\n",
        "\n",
        "Suppose we have 10 emails:\n",
        "\n",
        "6 spam (class 1)\n",
        "\n",
        "4 not spam (class 0)\n",
        "\n",
        "Entropy before any split:\n",
        "\n",
        "Entropy(S) = -(0.6 \\log_2 0.6 + 0.4 \\log_2 0.4) \\approx 0.971\n",
        "\n",
        "Now consider splitting on feature = \"Contains word 'Offer'?\"\n",
        "\n",
        "Subset 1 (Yes): 5 emails → 4 spam, 1 not spam → Entropy ≈ 0.722\n",
        "\n",
        "Subset 2 (No): 5 emails → 2 spam, 3 not spam → Entropy ≈ 0.971\n",
        "\n",
        "\n",
        "Weighted Entropy after split:\n",
        "\n",
        "= (5/10) \\times 0.722 + (5/10) \\times 0.971 = 0.846\n",
        "\n",
        "So,\n",
        "IG = 0.971 - 0.846 = 0.125\n",
        "\n",
        "This means the feature \"Contains 'Offer'\" reduces uncertainty by 0.125 bits.\n",
        "\n",
        "The Decision Tree will compare IG for all features and choose the highest IG split.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans Real-World Applications of Decision Trees\n",
        "\n",
        "1️⃣ Finance / Banking\n",
        "\n",
        "Loan Approval → Decide if a person is eligible for a loan based on income, credit score, past repayment history.\n",
        "\n",
        "Fraud Detection → Classify transactions as fraudulent or genuine.\n",
        "\n",
        "2️⃣ Healthcare\n",
        "\n",
        "Disease Diagnosis → Predict whether a patient has a disease based on symptoms, test results, and history.\n",
        "\n",
        "Treatment Recommendations → Choose treatment plans by analyzing patient data.\n",
        "\n",
        "3️⃣ Marketing / Business\n",
        "\n",
        "Customer Segmentation → Classify customers into groups (high value vs low value).\n",
        "\n",
        "Churn Prediction → Predict whether a customer will leave a service.\n",
        "\n",
        "4️⃣ Manufacturing\n",
        "\n",
        "Quality Control → Detect defective vs. non-defective products.\n",
        "\n",
        "Predictive Maintenance → Decide when machines are likely to fail.\n",
        "\n",
        "5️⃣ IT & Communication\n",
        "\n",
        "Spam Email Classification → Classify emails as spam or not spam.\n",
        "\n",
        "Recommendation Systems → E.g., suggesting products based on purchase behavior.\n",
        "\n",
        "Advantages of Decision Trees\n",
        "\n",
        "1. Easy to Understand & Interpret → Looks like flowcharts (good for business stakeholders).\n",
        "\n",
        "2. No Feature Scaling Required → Works with raw data (no need for normalization/standardization).\n",
        "\n",
        "3. Handles Both Categorical & Numerical Data.\n",
        "\n",
        "4. Non-linear Relationships → Can capture complex patterns.\n",
        "\n",
        "5. Fast & Low Computation → Compared to more complex models."
      ],
      "metadata": {
        "id": "ymy2yZqnL2ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Decision Tree Classifier using Gini\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Model Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pebu5xIpQgqv",
        "outputId": "d73e6939-769d-4f30-bdc3-bd9624295aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Fully-grown Decision Tree (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 4. Pre-pruned Decision Tree (max_depth=3)\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# 5. Print Results\n",
        "print(\"Accuracy of Fully-Grown Tree:\", accuracy_full)\n",
        "print(\"Accuracy of Pruned Tree (max_depth=3):\", accuracy_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Sihn1IRCmT",
        "outputId": "9be9b28d-77bd-438a-e2cf-ccd8fedb4d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-Grown Tree: 1.0\n",
            "Accuracy of Pruned Tree (max_depth=3): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Decision Tree Regressor (default = fully grown tree)\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Decision Tree Regressor MSE:\", mse)\n",
        "\n",
        "# 6. Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8n8ivUsRWYA",
        "outputId": "d6f053e6-a2af-4630-9711-505f7405cd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor MSE: 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,              # 5-fold cross-validation\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs98I-WQRzme",
        "outputId": "8e93153c-3cbc-4070-a601-1e837d549ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "Ans Disease Prediction Using Decision Trees\n",
        "1️⃣ Handle Missing Values\n",
        "\n",
        "Healthcare datasets almost always have missing values (e.g., lab tests not done, incomplete surveys).\n",
        "\n",
        "Numerical features:\n",
        "\n",
        "Replace with median (more robust than mean for skewed data).\n",
        "\n",
        "Or use KNN/Iterative Imputer if missingness depends on other variables.\n",
        "\n",
        "\n",
        "Categorical features:\n",
        "\n",
        "Fill with most frequent category.\n",
        "\n",
        "Or create a separate \"Unknown\" category (useful if missingness has meaning).\n",
        "\n",
        "\n",
        "Important note: Keep a record of missingness — in medicine, missing data can itself be informative (e.g., missing lab test could indicate low-risk patient).\n",
        "\n",
        "2️⃣ Encode Categorical Features\n",
        "\n",
        "Decision Trees can handle numeric splits, but categorical variables must be encoded.\n",
        "\n",
        "Ordinal variables (e.g., disease stage I < II < III): use Label Encoding.\n",
        "\n",
        "Nominal variables (e.g., blood type, gender, smoker/non-smoker): use One-Hot Encoding.\n",
        "\n",
        "Encoding ensures the model does not assume false numeric relationships.\n",
        "\n",
        "3️⃣ Train a Decision Tree Model\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Split data → Train (70%) / Test (30%).\n",
        "\n",
        "2. Initialize Decision Tree:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "3. Predictions → y_pred = clf.predict(X_test)\n",
        "\n",
        "4️⃣ Tune Hyperparameters\n",
        "\n",
        "Decision Trees tend to overfit if grown fully. We use GridSearchCV or RandomizedSearchCV to tune:\n",
        "\n",
        "max_depth → Maximum depth of the tree.\n",
        "min_samples_split → Minimum samples required to split a node.\n",
        "\n",
        "min_samples_leaf → Minimum samples required in a leaf.\n",
        "\n",
        "max_features → Number of features considered per split.\n",
        "\n",
        "Example tuning:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 7, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "5️⃣ Evaluate Performance\n",
        "\n",
        "In healthcare, accuracy alone is not enough. More critical metrics:\n",
        "\n",
        "Precision → Of predicted positives, how many were truly diseased? (important to reduce false alarms).\n",
        "\n",
        "Recall (Sensitivity) → Of actual diseased patients, how many did we catch? (critical in healthcare).\n",
        "\n",
        "F1-Score → Balance between precision & recall.\n",
        "\n",
        "ROC-AUC → Ability to distinguish between disease vs no-disease.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n",
        "\n",
        "6️⃣ Business Value in Real-World Healthcare\n",
        "\n",
        "Early Disease Detection → Doctors can identify high-risk patients earlier → improved patient survival.\n",
        "\n",
        "Resource Optimization → Prioritize expensive tests for high-risk patients, saving time & money.\n",
        "\n",
        "Personalized Treatment → Predictive models help tailor care pathways.\n",
        "\n",
        "Cost Reduction → Avoid unnecessary tests for low-risk patients.\n",
        "\n",
        "Improved Patient Trust → Consistent, data-driven predictions enhance confidence in diagnosis."
      ],
      "metadata": {
        "id": "sJVEZEyGTv4h"
      }
    }
  ]
}