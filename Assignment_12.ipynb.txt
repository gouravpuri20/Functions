{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function\n",
        "\n",
        "         SVM & Naive Bayes\n",
        "\n",
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Ans A Support Vector Machine (SVM) is a supervised machine learning algorithm mainly used for classification (and sometimes regression).\n",
        "\n",
        "The main idea:\n",
        " It tries to find the best possible decision boundary (hyperplane) that separates data points of different classes.\n",
        "How It Works\n",
        "\n",
        "1. Hyperplane Concept\n",
        "\n",
        "In 2D space, the hyperplane is just a line.\n",
        "\n",
        "In 3D space, it‚Äôs a plane.\n",
        "\n",
        "In higher dimensions, it‚Äôs called a hyperplane.\n",
        "\n",
        "SVM tries to find the hyperplane that best separates the classes.\n",
        "\n",
        "2. Maximizing the Margin\n",
        "\n",
        "SVM doesn‚Äôt just separate the classes‚Äîit finds the hyperplane with the maximum margin.\n",
        "\n",
        "Margin = the distance between the hyperplane and the closest data points from each class.\n",
        "\n",
        "The closest data points are called Support Vectors (they ‚Äúsupport‚Äù the boundary).\n",
        "\n",
        "The idea is: a wider margin = better generalization on unseen data.\n",
        "\n",
        "3. Handling Non-Linear Data\n",
        "\n",
        "What if the data isn‚Äôt linearly separable?\n",
        "\n",
        "SVM uses the Kernel Trick:\n",
        "\n",
        "Maps data into higher dimensions so it can become linearly separable.\n",
        "\n",
        "Common kernels: Linear, Polynomial, RBF (Gaussian).\n",
        "\n",
        "4. Soft Margin (for noisy data)\n",
        "\n",
        "Sometimes perfect separation isn‚Äôt possible.\n",
        "\n",
        "SVM allows some misclassifications by introducing a penalty (C parameter).\n",
        "\n",
        "This balances between maximizing margin and minimizing classification error.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Ans Hard Margin SVM\n",
        "Tries to find a hyperplane that perfectly separates the data without any misclassification.\n",
        "\n",
        "Works only when data is linearly separable (clean, no noise, no overlap).\n",
        "\n",
        "Very strict ‚Üí even one outlier can break the model.\n",
        "\n",
        "Risk: Overfitting if the data is noisy.\n",
        "Soft Margin SVM\n",
        "\n",
        "Allows some misclassifications (slack variables) to handle noise and overlap in data.\n",
        "\n",
        "Introduces a penalty parameter (C):\n",
        "\n",
        "Large C ‚Üí tries to classify every point correctly (behaves like hard margin).\n",
        "Small C ‚Üí allows more misclassifications but achieves better generalization.\n",
        "More practical in real-world datasets where perfect separation is rare\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Ans Kernel Trick in SVM\n",
        "\n",
        "Many datasets are not linearly separable (you can‚Äôt draw a straight line to separate classes).\n",
        "\n",
        "The Kernel Trick allows SVM to project the data into a higher-dimensional space where it becomes linearly separable, without explicitly computing that transformation (which would be very costly).\n",
        "\n",
        " In simple terms:\n",
        "The kernel trick is a mathematical shortcut that lets SVM classify non-linear data by working in higher dimensions implicitly.\n",
        "\n",
        "Example of a Kernel\n",
        "\n",
        "Radial Basis Function (RBF) Kernel (Gaussian Kernel)\n",
        "\n",
        "Formula:\n",
        "\n",
        "K(x, x') = \\exp(-\\gamma ||x - x'||^2)\n",
        "\n",
        "If two points are very close, the kernel value is high; if far apart, the value is near zero.\n",
        "\n",
        "Use Case\n",
        "\n",
        "Suppose you want to classify emails as spam or not spam.\n",
        "\n",
        "In the original space (based on word counts), spam vs non-spam might not be separable by a straight line.\n",
        "\n",
        "Using an RBF Kernel, SVM maps the data into a higher-dimensional space where the classes can be separated with a hyperplane.\n",
        "\n",
        "RBF is widely used when data is non-linear and complex.\n",
        "\n",
        "Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "Ans Na√Øve Bayes Classifier is a supervised machine learning algorithm used for classification problems.\n",
        "It is based on Bayes‚Äô Theorem, which deals with probability.\n",
        "\n",
        "The idea is:\n",
        "It calculates the probability of a class given some features and then predicts the class with the highest probability.\n",
        "\n",
        "üîπ Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "It is called na√Øve because it makes a very strong and unrealistic assumption:\n",
        "It assumes that all features are independent of each other, given the class label.\n",
        "\n",
        "For example, in email spam detection:\n",
        "\n",
        "Features = [\"word 'free' appears\", \"word 'money' appears\", \"word 'discount' appears\"].\n",
        "Na√Øve Bayes assumes that these words are completely independent.\n",
        "In reality, words are related (e.g., ‚Äúfree money‚Äù is a strong phrase), but still this assumption makes the algorithm very simple and fast.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Ans Na√Øve Bayes has different variants, depending on the type of data we are working with. The three main ones are:\n",
        "\n",
        "1. Gaussian Na√Øve Bayes\n",
        "\n",
        "Used for: Continuous (numeric) data.\n",
        "\n",
        "Assumption: The continuous values associated with each class follow a normal (Gaussian) distribution.\n",
        "\n",
        "Formula:\n",
        "\n",
        "\n",
        "P(x|C) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
        "\n",
        "where  = mean and  = variance of the feature for class .\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Predicting whether a person has a disease based on continuous attributes (like age, blood pressure, cholesterol level).\n",
        "\n",
        "Iris dataset classification (flower petal/sepal measurements).\n",
        "\n",
        "2. Multinomial Na√Øve Bayes\n",
        "\n",
        "Used for: Discrete data (counts, frequencies).\n",
        "\n",
        "Assumption: Features represent counts of events (e.g., word frequencies).\n",
        "\n",
        "Common in text classification where features are word counts in documents.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Spam detection (how many times words like ‚Äúfree‚Äù, ‚Äúwin‚Äù, ‚Äúmoney‚Äù appear).\n",
        "\n",
        "Document classification by topic (sports, politics, science).\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "Used for: Binary/boolean data (yes/no, 0/1).\n",
        "\n",
        "Assumption: Features are binary ‚Äî a feature is either present or absent.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Text classification where only word presence/absence matters (not how many times it appears).\n",
        "\n",
        "Example: Does the email contain the word ‚Äúoffer‚Äù? (Yes/No)\n",
        "\n",
        "Sentiment analysis (whether certain positive/negative words appear in a review)."
      ],
      "metadata": {
        "id": "ngw4GpvmQPM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and support vectors\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Number of Support Vectors for each class:\", svm_clf.n_support_)\n",
        "print(\"Support Vectors:\\n\", svm_clf.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx3MRd_7G3g5",
        "outputId": "93f877f6-cd50-4e79-d3af-83159a797909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Number of Support Vectors for each class: [ 3 11 10]\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 7: Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data     # Features\n",
        "y = data.target   # Labels\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDu45zaSHMba",
        "outputId": "ec860a0b-d131-4e8f-e3d3-20707e793440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 8: Write a Python program to:\n",
        "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "‚óè Print the best hyperparameters and accuracy.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.01, 0.1, 1, 10],\n",
        "    'kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "# Create GridSearchCV with SVM\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMSUpwl_Hqly",
        "outputId": "59fcf981-f7fe-4fce-c0cd-380c7fbc2572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9: Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data   # text documents\n",
        "y = newsgroups.target # labels (0 or 1)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Multinomial Na√Øve Bayes classifier\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on test set\n",
        "y_proba = nb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_wtdRUQIGiQ",
        "outputId": "60f22924-a054-4802-c0e9-ac589a9a4939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9999547378188155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Ans Step 1: Preprocessing the Data\n",
        "\n",
        "Handle Missing Data ‚Üí Replace missing values in text (e.g., empty body) with a placeholder like \"missing\".\n",
        "\n",
        "Text Vectorization ‚Üí Convert emails into numerical form using:\n",
        "\n",
        "Bag of Words (CountVectorizer) or\n",
        "\n",
        "TF-IDF (Term Frequency ‚Äì Inverse Document Frequency) ‚Üí better because it reduces the weight of common words (like \"the\", \"and\").\n",
        "\n",
        "Lowercasing, stopword removal, stemming/lemmatization ‚Üí improves text quality.\n",
        "\n",
        "Step 2: Choosing a Model\n",
        "\n",
        "Na√Øve Bayes (MultinomialNB) ‚Üí works well for text data, fast, interpretable.\n",
        "\n",
        "SVM (Support Vector Machine) ‚Üí handles high-dimensional data better, usually more accurate but slower.\n",
        "If speed & scalability is important ‚Üí Na√Øve Bayes\n",
        "If accuracy & robust classification is priority ‚Üí SVM\n",
        "\n",
        "Step 3: Handling Class Imbalance\n",
        "\n",
        "Since spam is fewer than ham:\n",
        "\n",
        "Class weights (class_weight='balanced' for SVM).\n",
        "\n",
        "Oversampling (SMOTE) or Undersampling.\n",
        "\n",
        "Threshold tuning after probability prediction.\n",
        "\n",
        "Step 4: Model Evaluation\n",
        "\n",
        "Use metrics beyond accuracy (since imbalance exists):\n",
        "\n",
        "Precision (How many predicted spam are actually spam).\n",
        "\n",
        "Recall (How many actual spam we catch).\n",
        "\n",
        "F1-score (balance between precision & recall).\n",
        "\n",
        "ROC-AUC (overall performance).\n",
        "\n",
        "Step 5: Business Impact\n",
        "\n",
        "Reduces spam emails reaching inbox ‚Üí saves employee time & increases productivity.\n",
        "\n",
        "Protects against phishing/malware ‚Üí reduces security risks.\n",
        "\n",
        "Improves customer trust ‚Üí legitimate emails are not lost."
      ],
      "metadata": {
        "id": "KQbawKePkcFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Sample dataset (for demo)\n",
        "data = {\n",
        "    'email': [\n",
        "        \"Win money now!!!\", \"Lowest price guarantee\", \"Meeting tomorrow at 10am\",\n",
        "        \"Your invoice is attached\", \"Earn $$$ fast\", \"Important project deadline\",\n",
        "        \"Cheap loans available\", \"Let's have lunch\", \"Update your bank details\"\n",
        "    ],\n",
        "    'label': [\"spam\",\"spam\",\"ham\",\"ham\",\"spam\",\"ham\",\"spam\",\"ham\",\"spam\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing values\n",
        "df['email'] = df['email'].fillna(\"missing\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['email'], df['label'],\n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# Vectorization (TF-IDF)\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# ----- Model 1: Naive Bayes -----\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "nb_preds = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Naive Bayes Classification Report:\")\n",
        "print(classification_report(y_test, nb_preds))\n",
        "\n",
        "# ----- Model 2: SVM (handles imbalance better) -----\n",
        "svm_model = LinearSVC(class_weight='balanced')\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_preds = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkhFR_Ev2oGT",
        "outputId": "9fdd857d-0f2d-4cee-d0ae-b8060451cc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.00      0.00      0.00         2\n",
            "        spam       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.00      0.00      0.00         2\n",
            "        spam       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}