{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function\n",
        "\n",
        "\n",
        "          Ensemble Learning\n",
        "\n",
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Ans Ensemble learning is a machine learning technique where multiple models (often called \"weak learners\") are combined to form a stronger and more accurate model. Instead of relying on a single model, an ensemble aggregates the predictions of several models to improve overall performance.\n",
        "\n",
        "Key Idea Behind Ensemble Learning:\n",
        "\n",
        "The main idea is that a group of diverse models can outperform any individual model if they are combined properly. Each model may make different errors, but when combined, these errors can cancel each other out, leading to better generalization and accuracy.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans 1.Basic concept\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Trains multiple models independently on different random subsets of the data.\n",
        "\n",
        "Combines their predictions using voting (for classification) or averaging (for regression).\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Trains multiple models sequentially, where each new model tries to correct the errors made by the previous ones.\n",
        "\n",
        "Gives more weight to misclassified samples in subsequent models.\n",
        "\n",
        "2. Purpose\n",
        "\n",
        "Bagging: Reduces variance (helps prevent overfitting).\n",
        "\n",
        "Boosting: Reduces bias (makes weak learners stronger).\n",
        "\n",
        "3. Data Sampling\n",
        "\n",
        "Bagging: Uses bootstrap sampling (random sampling with replacement).\n",
        "\n",
        "Boosting: Uses the entire dataset but updates the weights of observations based on previous model errors.\n",
        "\n",
        "4. Model Training\n",
        "\n",
        "Bagging: Models are trained in parallel (independent of each other).\n",
        "\n",
        "Boosting: Models are trained sequentially (each depends on the previous one).\n",
        "\n",
        "5. Model Weighting\n",
        "\n",
        "Bagging: All models are given equal weight in final prediction.\n",
        "\n",
        "Boosting: Models are given different weights; later models have higher impact.\n",
        "\n",
        "6. Risk of Overfitting\n",
        "\n",
        "Bagging: Less prone to overfitting.\n",
        "\n",
        "Boosting: More prone to overfitting if not regularized properly.\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Ans Bootstrap sampling is a statistical technique where we randomly select samples from the original dataset with replacement to create multiple new datasets (called bootstrap samples) of the same size as the original dataset.\n",
        "\n",
        "With replacement means that after picking a data point, it is put back into the dataset, so it can be selected again.\n",
        "\n",
        "As a result, some records appear multiple times in the bootstrap sample, and some may not appear at all.\n",
        "\n",
        "Example of Bootstrap Sampling:\n",
        "\n",
        "Suppose you have a dataset with 5 samples:\n",
        "[A, B, C, D, E]\n",
        "A bootstrap sample could be:\n",
        "[B, A, D, D, E] (notice D appears twice, C is missing).\n",
        "\n",
        "By repeating this process, we create multiple bootstrap datasets.\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging (e.g., Random Forest):\n",
        "\n",
        "1. Create Diversity Among Models:\n",
        "\n",
        "Bagging methods like Random Forest train multiple decision trees.\n",
        "\n",
        "Each tree is trained on a different bootstrap sample of the dataset.\n",
        "\n",
        "This ensures each model sees slightly different data, leading to diverse predictions.\n",
        "\n",
        "2. Reduce Variance & Overfitting:\n",
        "\n",
        "Since each tree is trained on different data, their errors are less correlated.\n",
        "\n",
        "When we average their predictions (for regression) or take majority voting (for classification), the variance decreases and overfitting reduces.\n",
        "\n",
        "3. Enables Out-of-Bag (OOB) Error Estimation:\n",
        "\n",
        "About 63% of the original samples appear in any given bootstrap sample (on average), and the remaining ~37% are left out.\n",
        "\n",
        "These left-out samples are called out-of-bag samples and can be used to estimate model performance without needing a separate validation set.\n",
        "\n",
        "In Random Forest specifically:\n",
        "\n",
        "Each tree is trained on a different bootstrap sample.\n",
        "\n",
        "Additionally, at each split in the tree, a random subset of features is also considered, adding even more randomness and diversity.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Ans Out-of-Bag (OOB) Samples:\n",
        "\n",
        "When we use bootstrap sampling (sampling with replacement) to create training subsets for each model in a bagging method (like Random Forest), not all data points from the original dataset are included in each bootstrap sample.\n",
        "\n",
        "On average, about 63% of the original data points appear in each bootstrap sample.\n",
        "\n",
        "The remaining ~37% that are not selected are called Out-of-Bag (OOB) samples for that particular model.\n",
        "\n",
        "Each tree in a Random Forest has its own OOB samples, which are different from the OOB samples of other trees.\n",
        "\n",
        "How Are OOB Samples Used?\n",
        "\n",
        "1. OOB Prediction:\n",
        "\n",
        "After a tree is trained on its bootstrap sample, the OOB samples (data not seen by that tree) are used to make predictions with that tree.\n",
        "\n",
        "For each data point, predictions are aggregated only from the trees for which that point was OOB.\n",
        "\n",
        "2. OOB Score (Evaluation Metric):\n",
        "\n",
        "The OOB score is essentially the accuracy (or another relevant metric) calculated using only OOB predictions.\n",
        "\n",
        "It serves as an internal validation method—no separate validation/test set is required.\n",
        "\n",
        "Formula for OOB Score:\n",
        "\n",
        "For classification:\n",
        "\n",
        "OOB\\ Score = \\frac{\\text{Number of correctly predicted OOB samples}}{\\text{Total number of samples}}\n",
        "\n",
        "For regression:\n",
        "\n",
        "Mean Squared Error (MSE) or  can be calculated using OOB predictions.\n",
        "\n",
        "Benefits of OOB Score:\n",
        "\n",
        "1. No Need for a Separate Validation Set:\n",
        "It provides a reliable internal estimate of model performance.\n",
        "\n",
        "2. Efficient Use of Data:\n",
        "Since all data points are used for training and validation (some trees train on them, others validate on them), it maximizes available data.\n",
        "\n",
        "3. Early Stopping & Model Tuning:\n",
        "The OOB score can help you choose the right number of trees or tune hyperparameters.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Ans Feature Importance in a Single Decision Tree\n",
        "\n",
        "1. How It’s Calculated:\n",
        "\n",
        "A Decision Tree calculates feature importance based on how much each feature reduces impurity (e.g., Gini Index or Entropy for classification, variance for regression).\n",
        "\n",
        "At each split, the feature that produces the highest decrease in impurity is chosen.\n",
        "\n",
        "The importance score for a feature is the sum of impurity decreases it contributes across all nodes where it is used, normalized so all feature importances add up to 1.\n",
        "\n",
        "2. Characteristics:\n",
        "\n",
        "Reflects only one tree’s structure, which can make it unstable (a small change in data can lead to a completely different tree and thus different feature importance).\n",
        "\n",
        "The importance might be biased toward features with many levels (e.g., categorical features with many categories).\n",
        "\n",
        "Feature Importance in a Random Forest\n",
        "\n",
        "1. How It’s Calculated:\n",
        "\n",
        "A Random Forest is an ensemble of multiple trees, and feature importance is averaged across all trees.\n",
        "\n",
        "Two common methods:\n",
        "\n",
        "Mean Decrease in Impurity (MDI): Average of impurity reductions contributed by each feature over all trees.\n",
        "\n",
        "Mean Decrease in Accuracy (MDA): Measures how much model accuracy decreases when the feature’s values are randomly permuted (permuta­tion importance).\n",
        "\n",
        "2. Characteristics:\n",
        "\n",
        "More robust and stable because it averages results across many trees.\n",
        "\n",
        "Less sensitive to small changes in data compared to a single tree.\n",
        "\n",
        "Provides a more generalizable measure of which features are truly important for the overall prediction task."
      ],
      "metadata": {
        "id": "dIciPGhKPyXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# 3. Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Combine feature names and their importances into a DataFrame for better readability\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance (descending order)\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# 4. Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA7mAaCuUhH6",
        "outputId": "043b3c38-a51e-417a-dd3c-f2ae35037ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_preds = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# 4. Train a Bagging Classifier using Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42), # use base_estimator for older sklearn versions\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_preds = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 5. Print and compare accuracies\n",
        "print(f\"Accuracy of single Decision Tree: {dt_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {bagging_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLdTCTxbV8Q9",
        "outputId": "0b7a4c12-859e-4d68-d78d-db70a8415e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset (Iris dataset used as an example)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# 5. Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1             # Use all available cores\n",
        ")\n",
        "\n",
        "# 6. Fit the grid search on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Get the best parameters and evaluate on test data\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 8. Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp-yasvIWOO7",
        "outputId": "953337d3-700b-437d-f85c-2f9bc7a1a948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Bagging Regressor (using DecisionTreeRegressor as base estimator)\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),  # use base_estimator for older sklearn versions\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_preds = bagging_reg.predict(X_test)\n",
        "\n",
        "# 4. Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_preds = rf_reg.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Errors\n",
        "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# 6. Print results\n",
        "print(f\"Bagging Regressor MSE: {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKFAsFDWpUO",
        "outputId": "41e17395-5e96-460c-cb24-7da4ebd19b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2559\n",
            "Random Forest Regressor MSE: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "Ans Step 1: Choosing between Bagging and Boosting\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Reduces variance by training multiple models on different random subsets of data.\n",
        "\n",
        "Good when the base model is high variance (e.g., Decision Trees).\n",
        "\n",
        "Models run in parallel, so faster to train.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Sequentially trains models, each focusing on the errors of the previous model.\n",
        "\n",
        "Reduces bias and variance, but more prone to overfitting if not tuned.\n",
        "\n",
        "Slower since models depend on previous ones.\n",
        "Decision in this context:\n",
        "\n",
        "Predicting loan default is often imbalanced (few defaults).\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM, CatBoost) is usually preferred because:\n",
        "\n",
        "It handles imbalanced data well.\n",
        "\n",
        "Captures complex relationships in transactional and demographic data.\n",
        "\n",
        "Often achieves higher predictive accuracy than Bagging.\n",
        "\n",
        "Step 2: Handling Overfitting\n",
        "\n",
        "Even ensemble models can overfit if the model is too complex or the dataset is small. Techniques include:\n",
        "\n",
        "1. Cross-validation: Use k-fold CV to ensure model generalizes.\n",
        "\n",
        "\n",
        "2. Hyperparameter tuning:\n",
        "\n",
        "For boosting: max_depth, min_child_weight, learning_rate, n_estimators.\n",
        "\n",
        "For bagging: max_samples, max_features.\n",
        "3. Regularization:\n",
        "\n",
        "L1/L2 regularization in boosting.\n",
        "\n",
        "Pruning in decision trees.\n",
        "\n",
        "4. Early stopping: Stop training if validation performance stops improving.\n",
        "\n",
        "\n",
        "5. Feature selection: Remove irrelevant or highly correlated features to reduce noise.\n",
        "\n",
        "Step 3: Selecting Base Models\n",
        "\n",
        "Ensemble performance depends on base models.\n",
        "\n",
        "For Bagging, typical base model: Decision Trees (high variance).\n",
        "\n",
        "For Boosting, base models are usually weak learners like shallow decision trees.\n",
        "\n",
        "Can also experiment with:\n",
        "\n",
        "Random Forest (Bagging variant)\n",
        "\n",
        "Gradient Boosting (XGBoost/LightGBM/CatBoost)\n",
        "\n",
        "Note: Always start simple and gradually increase complexity.\n",
        "\n",
        "Step 4: Evaluate Performance using Cross-Validation\n",
        "\n",
        "Process:\n",
        "\n",
        "1. Split data: Use Stratified K-Fold CV to maintain class balance (important for default prediction).\n",
        "\n",
        "2. Metrics:\n",
        "\n",
        "Accuracy is not enough (dataset imbalanced).\n",
        "\n",
        "Prefer: Precision, Recall, F1-score, AUC-ROC.\n",
        "\n",
        "3. Pipeline:\n",
        "\n",
        "For each fold:\n",
        "    - Train model on training split\n",
        "    - Evaluate on validation split\n",
        "    - Record metrics\n",
        "Aggregate metrics across folds\n",
        "\n",
        "4. Hyperparameter tuning: Use Grid Search or Randomized Search with CV.\n",
        "\n",
        "Step 5: How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "In a financial context:\n",
        "\n",
        "1. Higher predictive accuracy: Reduces false positives/negatives, meaning fewer risky loans approved.\n",
        "\n",
        "2. Robustness: Combining models reduces impact of a single biased model.\n",
        "\n",
        "3. Captures complex patterns: Boosting can detect subtle risk signals in transactional behavior.\n",
        "\n",
        "4. Supports risk-based decisions:\n",
        "\n",
        "Assign risk scores to customers.\n",
        "\n",
        "Helps credit officers decide loan terms, interest rates, or rejection.\n",
        "\n",
        "5. Regulatory compliance: Reliable models with CV-backed metrics support audit requirements.\n",
        "\n",
        "Bottom line: Ensemble models improve both accuracy and trust in automated loan decisions, reducing defaults and increasing revenue while maintaining regulatory compliance."
      ],
      "metadata": {
        "id": "EiIfb0u1YDr_"
      }
    }
  ]
}