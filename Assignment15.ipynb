{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function\n",
        "\n",
        "          Boosting Techniques\n",
        "\n",
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Ans Boosting is an ensemble learning technique that combines multiple weak learners (usually simple models like shallow decision trees) to create a strong predictive model.\n",
        "\n",
        "It works sequentially, meaning each new model tries to correct the errors made by the previous models.\n",
        "\n",
        "The final prediction is a weighted combination of all weak learners.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "> A weak learner might perform slightly better than random guessing. Boosting “boosts” its performance by combining many weak learners into a strong learner.\n",
        "\n",
        "How Boosting Improves Weak Learners\n",
        "\n",
        "1. Sequential learning:\n",
        "\n",
        "Each new model focuses more on the misclassified examples from the previous model.\n",
        "\n",
        "This reduces the overall error step by step.\n",
        "\n",
        "2. Weighted contribution:\n",
        "\n",
        "Weak learners that perform better get higher weight in the final model.\n",
        "\n",
        "3. Error reduction:\n",
        "\n",
        "By correcting mistakes iteratively, boosting reduces bias and variance simultaneously.\n",
        "\n",
        "4. Adaptability:\n",
        "\n",
        "Boosting adapts to difficult patterns in the data that a single weak learner might miss.\n",
        "\n",
        "Popular Boosting Algorithms\n",
        "\n",
        "AdaBoost (Adaptive Boosting): Focuses on misclassified points by updating their weights.\n",
        "\n",
        "Gradient Boosting: Learns from the residuals (errors) of previous models.\n",
        "\n",
        "XGBoost / LightGBM / CatBoost: Optimized versions of Gradient Boosting with better speed, handling of missing values, and regularization.\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Ans 1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Training approach: Sequentially trains weak learners (usually decision stumps).\n",
        "\n",
        "Focus: After each weak learner, it increases the weight of misclassified samples so that the next model focuses more on them.\n",
        "\n",
        "Error handling: Misclassified instances get higher importance in the next iteration.\n",
        "\n",
        "Optimization: Minimizes exponential loss function.\n",
        "\n",
        "Prediction combination: Weighted majority vote (for classification) or weighted sum (for regression) of weak learners.\n",
        "\n",
        "\n",
        "Step-by-step idea:\n",
        "\n",
        "1. Train a weak learner on the dataset.\n",
        "\n",
        "\n",
        "2. Measure errors for each instance.\n",
        "\n",
        "\n",
        "3. Increase weights of misclassified points.\n",
        "\n",
        "\n",
        "4. Train next weak learner on the re-weighted data.\n",
        "\n",
        "\n",
        "5. Combine all weak learners using weighted voting.\n",
        "\n",
        "2. Gradient Boosting\n",
        "\n",
        "Training approach: Sequentially trains weak learners, but each learner tries to fit the residual errors (gradients) of the previous model.\n",
        "\n",
        "Focus: Instead of reweighting samples, it fits a new model to reduce the residual errors of prior models.\n",
        "\n",
        "Error handling: Learner focuses on predicting what the previous ensemble is missing.\n",
        "\n",
        "Optimization: Minimizes a differentiable loss function using gradient descent.\n",
        "\n",
        "Prediction combination: Models are summed together to make the final prediction (additive model).\n",
        "\n",
        "\n",
        "Step-by-step idea:\n",
        "\n",
        "1. Train an initial model (like a decision tree) on the dataset.\n",
        "\n",
        "\n",
        "2. Compute the residuals (difference between actual and predicted).\n",
        "\n",
        "\n",
        "3. Train a new weak learner to predict these residuals.\n",
        "\n",
        "\n",
        "4. Add this new learner to the ensemble with a learning rate.\n",
        "\n",
        "\n",
        "5. Repeat for a number of iterations to minimize overall loss.\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Ans XGBoost is an optimized implementation of Gradient Boosting. It builds trees sequentially to minimize loss but adds regularization to prevent overfitting.\n",
        "\n",
        "\n",
        " Role of Regularization in XGBoost\n",
        "\n",
        "XGBoost uses both L1 and L2 regularization on the weights of leaves in decision trees:\n",
        "\n",
        "1. L1 regularization (Lasso-like):\n",
        "\n",
        "Penalizes the absolute value of leaf weights.\n",
        "\n",
        "Encourages sparsity → some leaf weights become zero → simpler trees.\n",
        "\n",
        "2. L2 regularization (Ridge-like):\n",
        "\n",
        "Penalizes the squared value of leaf weights.\n",
        "\n",
        "Prevents very large weights → reduces overfitting.\n",
        "\n",
        "\n",
        " How it Helps\n",
        "\n",
        "Controls model complexity: By penalizing large weights or too many leaf splits, the model stays simpler.\n",
        "\n",
        "Prevents overfitting: Regularization ensures the model doesn’t memorize the training data.\n",
        "\n",
        "Improves generalization: Makes the model perform better on unseen data.\n",
        "\n",
        "Balances fit vs. complexity: The objective function in XGBoost becomes:\n",
        "\n",
        "\n",
        "\\text{Obj} = \\text{Training Loss} + \\text{Regularization Term (L1 + L2)}\n",
        "\n",
        "Training loss ensures accuracy, while regularization penalizes complexity.\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Ans CatBoost is a gradient boosting algorithm developed by Yandex, specifically designed to handle categorical features efficiently without needing extensive preprocessing like one-hot encoding.\n",
        "\n",
        "How CatBoost Handles Categorical Data\n",
        "\n",
        "1. Native Categorical Feature Support:\n",
        "\n",
        "Instead of converting categories into one-hot or label-encoded vectors, CatBoost directly works with categorical features.\n",
        "\n",
        "This avoids high-dimensional sparse matrices, which are memory-heavy and slow to train.\n",
        "\n",
        "2. Ordered Target Statistics (Mean Encoding):\n",
        "\n",
        "CatBoost uses a technique called “ordered boosting”:\n",
        "\n",
        "It calculates statistics (like mean target value) for each category without leaking target information from the current row.\n",
        "\n",
        "This reduces overfitting, which is common with naive target encoding.\n",
        "\n",
        "3. Efficient Feature Combination:\n",
        "\n",
        "CatBoost automatically generates combinations of categorical features to capture interactions without manual effort.\n",
        "\n",
        "4. GPU & CPU Optimizations:\n",
        "\n",
        "CatBoost has optimized algorithms to process categorical features efficiently, making it faster than traditional gradient boosting with one-hot encoding.\n",
        "\n",
        "Benefits of CatBoost for Categorical Data\n",
        "\n",
        "Feature Benefit\n",
        "\n",
        "No manual encoding needed   Saves preprocessing time and reduces errors\n",
        "Ordered target statistics   Prevents overfitting while using target information\n",
        "Handles high-cardinality data   Efficiently deals with categories with many unique values\n",
        "Automatic feature combinations  Captures interactions without manual feature engineering\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Ans 1. Key Difference Reminder\n",
        "\n",
        "Bagging (e.g., Random Forest): Reduces variance by averaging multiple independent models → works well when individual models are high variance.\n",
        "\n",
        "Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, CatBoost): Reduces bias by sequentially improving weak learners → works well when individual models are weak and need to be strong.\n",
        "\n",
        "Boosting is preferred when improving accuracy and handling complex relationships is more important than reducing variance alone.\n",
        "\n",
        "2. Real-World Applications of Boosting\n",
        "\n",
        "1. Financial Services\n",
        "\n",
        "Credit risk scoring / Loan default prediction\n",
        "\n",
        "Boosting captures subtle patterns in customer behavior, transaction history, and demographics.\n",
        "\n",
        "Example: Predicting which customers are likely to default on loans using XGBoost.\n",
        "\n",
        "2. E-commerce & Marketing\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Boosting handles imbalanced datasets and complex interactions between features (e.g., purchase frequency, product categories).\n",
        "\n",
        "3. Healthcare\n",
        "\n",
        "Disease prediction / Diagnosis\n",
        "\n",
        "Boosting can model subtle patterns in lab results, patient history, and demographics to detect diseases like diabetes or heart disease early.\n",
        "\n",
        "4. Fraud Detection\n",
        "\n",
        "Credit card fraud / Insurance claims fraud\n",
        "\n",
        "Boosting excels at imbalanced classification where fraudulent cases are rare but critical.\n",
        "\n",
        "5. Predictive Maintenance / Manufacturing\n",
        "\n",
        "Predicting machine failures or defects based on sensor data\n",
        "\n",
        "Boosting improves accuracy in datasets with many features and complex interactions.\n",
        "\n",
        "6. Text & NLP Applications\n",
        "\n",
        "Sentiment analysis or spam detection\n",
        "\n",
        "Boosting works well with engineered features from text data (TF-IDF, embeddings).\n",
        "\n",
        "3. Why Boosting Works Better Here\n",
        "\n",
        "Focuses on hard-to-predict instances.\n",
        "\n",
        "Captures non-linear relationships and feature interactions.\n",
        "\n",
        "Works well with imbalanced datasets.\n",
        "\n",
        "Usually achieves higher accuracy than bagging in complex prediction problems."
      ],
      "metadata": {
        "id": "dTcLCXm_bXut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "adaboost_model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Tgp_Kx-qiF",
        "outputId": "848350d8-474e-45cb-cbd1-b7a610a74d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuO3UcW8_Hx0",
        "outputId": "dcf93336-4bcd-4017-f3e2-0f1eb238a0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the grid of learning rates to search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best learning rate\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF0XxUf3_adO",
        "outputId": "655fb542-2cce-434b-b6c8-5df07da5e463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Set Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:46:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=3,\n",
        "    verbose=0  # Suppress output\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "YQu6ho4KA8ah",
        "outputId": "393870fc-9c3f-42c0-947e-574f2840364c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3331293089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Ans 1.Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "a. Missing Values\n",
        "\n",
        "Numeric features: Impute using median (robust to outliers) or use KNN imputation.\n",
        "\n",
        "Categorical features: Impute using the most frequent category or “Unknown” label.\n",
        "\n",
        "Keep track of imputed values for transparency in financial models.\n",
        "\n",
        "\n",
        "b. Categorical Features\n",
        "\n",
        "CatBoost: Can handle categorical features directly — no need for one-hot encoding.\n",
        "\n",
        "XGBoost/LightGBM: Convert to numerical via:\n",
        "\n",
        "One-hot encoding (for low-cardinality categories)\n",
        "\n",
        "Target encoding (for high-cardinality categories, while avoiding leakage)\n",
        "\n",
        "c. Feature Scaling\n",
        "\n",
        "Boosting models are tree-based, so scaling is generally not required.\n",
        "\n",
        "\n",
        "d. Handling Class Imbalance\n",
        "\n",
        "Since loan default is rare, the dataset is imbalanced:\n",
        "\n",
        "Use class weights in boosting algorithms (scale_pos_weight in XGBoost, class_weights in CatBoost).\n",
        "\n",
        "Resampling: SMOTE (synthetic minority oversampling) or undersampling majority class.\n",
        "\n",
        "Prefer metrics that handle imbalance (see evaluation metrics below).\n",
        "\n",
        "2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "Algorithm   Pros    Cons\n",
        "\n",
        "AdaBoost Simple, fast for small datasets Sensitive to noise, may underperform on complex patterns\n",
        "XGBoost Fast, supports missing values, robust, highly tunable   Needs numeric encoding for categorical variables\n",
        "CatBoost    Handles categorical features natively, robust to overfitting, good default performance  Slightly slower on CPU, but excellent for mixed data types\n",
        "\n",
        "Choice for this scenario: CatBoost\n",
        "\n",
        "Reason: Handles categorical features directly, robust to missing values, and usually performs well on imbalanced datasets.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Key Hyperparameters for CatBoost\n",
        "\n",
        "iterations: Number of trees\n",
        "\n",
        "depth: Depth of each tree\n",
        "\n",
        "learning_rate: Step size for boosting\n",
        "\n",
        "l2_leaf_reg: L2 regularization to reduce overfitting\n",
        "\n",
        "class_weights: To handle imbalance\n",
        "\n",
        "\n",
        "Tuning Strategy\n",
        "\n",
        "1. Use RandomizedSearchCV or GridSearchCV with 5-fold cross-validation.\n",
        "\n",
        "\n",
        "2. Start with learning_rate=0.1 and depth=6 (default).\n",
        "\n",
        "\n",
        "3. Tune depth and learning_rate together; larger depth increases complexity.\n",
        "\n",
        "\n",
        "4. Monitor AUC-ROC or F1-score (better for imbalanced data).\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Recommended Metrics:\n",
        "\n",
        "ROC-AUC: Measures model’s ability to rank positive vs negative cases, insensitive to class imbalance.\n",
        "\n",
        "Precision, Recall, F1-score: Crucial because false negatives (predicting no default when customer defaults) can cost the company money.\n",
        "\n",
        "Confusion Matrix: Visual understanding of type I/II errors.\n",
        "\n",
        "PR-AUC (Precision-Recall AUC): Especially useful if the positive class (default) is very rare.\n",
        "\n",
        "\n",
        "Why: In finance, catching defaults (high recall) is often more important than overall accuracy.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "How the business benefits:\n",
        "\n",
        "Risk reduction: Identify high-risk borrowers early to prevent financial loss.\n",
        "\n",
        "Targeted intervention: Offer counseling, adjust loan terms, or monitor transactions for high-risk customers.\n",
        "\n",
        "Optimized portfolio: Reduce non-performing loans (NPL) ratio.\n",
        "\n",
        "Regulatory compliance: Transparent models with interpretable features can satisfy auditors.\n",
        "\n",
        "Revenue optimization: By safely approving more low-risk loans, business can increase lending volume without increasing default risk."
      ],
      "metadata": {
        "id": "AfIZLchAChEa"
      }
    }
  ]
}
